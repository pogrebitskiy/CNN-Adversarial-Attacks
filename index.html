<!doctype html>
<html lang="en">
<head>
    <title>Protecting CV Models Against Adversarial Attacks</title>
    <meta property="og:title" content="Protecting CV Models Against Adversarial Attacks"/>
    <meta name="twitter:title" content="Protecting CV Models Against Adversarial Attacks"/>
    <meta name="description" content="Your project about your cool topic described right here."/>
    <meta property="og:description" content="Your project about your cool topic described right here."/>
    <meta name="twitter:description" content="Your project about your cool topic described right here."/>
    <meta property="og:type" content="website"/>
    <meta name="twitter:card" content="summary"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <!-- bootstrap for mobile-friendly layout -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
          integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
            integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
            crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
            crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
    <link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
    <div class="container">
        <h1 class="lead">
            <nobr class="widenobr">Adversarial Robustness of Different Network Structures</nobr>
            <nobr class="widenobr">For DS 4440</nobr>
        </h1>
    </div>
</div><!-- end nd-pageheader -->

<div class="container">
    <div class="row">
        <div class="col justify-content-center text-center">
            <h2>An Analysis of <i>Towards Deep Learning Models Resistant to Adversarial
                Attacks</i></h2>
            <p>By: David Pogrebitskiy, Benjamin Wyant</p>
            <a href="https://github.com/pogrebitskiy/CNN-Adversarial-Attacks" target="_blank">GitHub</a>
            <p></p>
        </div>
    </div>
    <div class="row">
        <div class="col">

            <h2>Introduction</h2>

            <p>Computer Vision models have become very reliable classifiers of complex image data, however, they have
                been shown to be quite easily tricked into misclassifying slightly altered data.
                This paper tackles a critical challenge in the field of deep learning - the vulnerability of neural
                networks to adversarial attacks.
                Adversarial examples are inputs that are almost indistinguishable from natural data, yet are classified
                incorrectly by the network with high confidence.
                These attacks not only pose security implications, but also reveal that our current models are not
                necessarily generalizing as well as we would like.</p>

            <p>We recreated the experiments by training five of our own models on the MNIST dataset and tested their
                adversarial robustness using PGD, BIM, and FGSM
                to generate adversarial examples. Our goal was to test a wider range of network architectures and
                determine how large of a role the structure
                plays in the model's ability to generalize better, thus being more robust to adversarial attacks.
            </p>

            <p>Achieving adversarial robustness is an important step towards making networks more secure and reliable,
                especially in high-stakes applications like autonomous driving and malware detection.
                The insights from this work could pave the way for a new generation of deep neural networks that are
                much more resistant adversarial attacks.</p>

            <h2>Paper Overview</h2>
            <p>The key challenge addressed in this paper is the vulnerability of deep neural networks to adversarial
                examples
                ,inputs that are almost indistinguishable from natural data, yet are classified incorrectly.</p>

            <p>
                Through different experiments, the authors demonstrate that finding adversarial examples has a
                computationally efficient solution.
                With simple first-order methods like projected gradient descent (PGD) reliably finding local maxima with
                similar loss values. Leveraging these insights,
                the authors trained neural networks on MNIST and CIFAR10 datasets that achieve high robustness against a
                wide range of adversarial attacks,
                including iterative methods like PGD. This represents a significant advance over prior work, which was
                often vulnerable to more sophisticated adversaries.
            </p>

            <h3>Bibliography</h3>

            <div class="image-container">
                <img src="images/Aleksander_Madry.png" alt="Image-bib">
                <p>
                    <b>Aleksander Madry</b>
                    Cadence Design Systems Professor of Computing in the MIT EECS Department.
                    Director of the MIT Center for Deployable Machine Learning and a Faculty Co-Lead of the MIT AI
                    Policy Forum.
                </p>
            </div>

            <div class="image-container">
                <img src="images/avatar_scholar.png" alt="Image-bib">
                <p>
                    <b>Aleksandar Makelov</b>
                    Independent researcher working on mechanistic interpretability of large language models.
                    Did his PhD from MIT.
                </p>
            </div>

            <div class="image-container">
                <img src="images/ludwig_schmidt.jpeg" alt="Image-bib">
                <p>
                    <b>Ludwig Schmidt</b>
                    Assistant professor in computer science at the University of Washington.
                    He is also a research scientist in the AllenNLP team at AI2 and a member of LAION.
                </p>
            </div>

            <div class="image-container">
                <img src="images/tsipras.jpeg" alt="Image-bib">
                <p>
                    <b>Dimitris Tsipras</b>
                    He is currently a research scientist at OpenAI.
                    Before that, he was a postdoc at Stanford CS and did his PhD in CS at MIT.
                </p>
            </div>

            <div class="image-container">
                <img src="images/vladu.png" alt="Image-bib">
                <p>
                    <b>Adrian Vladu</b>
                    He is a permanent researcher at IRIF, affiliated with CNRS and Université Paris Cité.
                    Before that, he received his PhD from MIT Math in 2017, which was followed by a postdoc at Boston
                    University.
                </p>
            </div>

            <h2>Methods</h2>
            <h4>Model Training</h4>
            <ul>
                <li>
                    <strong>2-layer fully connected:</strong>
                    This network is a simple model with one hidden layer between the input and output layers.
                    This model was used as a baseline for testing adversarial robustness against more complex
                    architectures.
                </li>
                <li>
                    <strong>VGG:</strong>
                    VGG (Visual Geometry Group) is a deep convolutional neural network known for its simplicity and
                    effectiveness.
                    It consists of multiple convolutional layers followed by max-pooling layers, with progressively
                    increasing depth.
                </li>
                <li>
                    <strong>LeNet:</strong>
                    LeNet is one of the earliest convolutional neural networks, designed for handwritten digit
                    recognition.
                    It comprises several layers of convolution, max-pooling, and fully connected layers.
                </li>
                <li>
                    <strong>GoogLeNet:</strong>
                    GoogLeNet, also known as Inception v1, is a deep convolutional neural network famous for its
                    inception modules,
                    which allow for efficient training and better performance by using multiple parallel convolutional
                    operations at each layer.
                </li>
                <li>
                    <strong>ResNet:</strong>
                    ResNet (Residual Network) introduced residual connections, enabling the training of very deep neural
                    networks
                    by mitigating the vanishing gradient problem. It features skip connections that add the input to the
                    output of a deeper layer,
                    facilitating the flow of gradients during training.
                </li>
            </ul>

            <p>
                We picked these models because each architecture contributed a substantial amount to the improvement of
                CNNs and computer vision.
                We wanted to see which architecture might be the most resistant to adversarial attacks.
            </p>

            <div style="max-width: 100%;">
                <img src="figures/training/MNIST_VGG.png" alt="Training" style="max-width: 100%;"/>
                <p style="text-align: center;">Accuracy and loss for VGG model trained on MNIST data</p>
            </div>

            <p>
                Each model was trained from scratch for 50 epochs on the MNIST dataset. We then tested each model's
                performance on
                the raw test set and adversarial adversarial images from each of the 3 types of attacks (PGD, BIM,
                FGSM).
            </p>

            <h4>Adversarial Training</h4>
            <p>After evaluating the original and augmented test sets on the models, we further trained each model for an
                additional 10 epochs
                using an augmented dataset. This new training data comprised a 50/50 split between original data and
                augmented images generated
                using each of the three adversarial attacks (PGD, BIM, and FGSM), resulting in 5 distinct models trained
                on 3 datasets each,
                totaling 15 models with their respective trained parameters.
            </p>

            <div style="max-width: 100%;">
                <img src="figures/testing/PGD_attacks.png" alt="Examples" style="max-width: 100%;"/>
                <p style="text-align: center;">PGD adversarial examples</p>
            </div>

            <a>Code testing our models can be found in attacks/attack_MNIST_Models.ipynb</a>

            <h2>Experimental Findings</h2>
            <p>
                After incorporating adversarial examples into our training process, we observed a slight decrease in
                the overall accuracy of our model on the original dataset. However, this reduction in accuracy was
                accompanied by a significant increase in adversarial robustness. By exposing the model to adversarial
                perturbations during training, it became more resilient to such attacks, demonstrating improved
                performance
                when faced with previously unseen adversarial inputs. This trade-off between accuracy on
                benign examples and robustness against adversarial attacks underscores the utility of incorporating
                adversarial training techniques.
            </p>
            <p>EPS = 0.2 - The EPS is the maximum perturbation size (L-inf)</p>
            <div class="table-container">
                <table class="tableizer-table">
                    <caption>Original model accuracy against each adversarial attack</caption>
                    <thead>
                    <tr>
                        <th>Model/Accuracy</th>
                        <th>Original Data</th>
                        <th>FGSM Adversarial</th>
                        <th>BIM Adversarial</th>
                        <th>PGD Adversarial</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>2-Layer Net</td>
                        <td>0.9884</td>
                        <td>0.5251</td>
                        <td>0.1897</td>
                        <td>0.1897</td>
                    </tr>
                    <tr>
                        <td>VGG</td>
                        <td><b>0.9951</b></td>
                        <td>0.7865</td>
                        <td>0.2252</td>
                        <td>0.2252</td>
                    </tr>
                    <tr>
                        <td>LeNet</td>
                        <td>0.9927</td>
                        <td><b>0.8184</b></td>
                        <td>0.3073</td>
                        <td>0.3073</td>
                    </tr>
                    <tr>
                        <td>GoogLeNet</td>
                        <td>0.994</td>
                        <td>0.7823</td>
                        <td>0.3659</td>
                        <td>0.3659</td>
                    </tr>
                    <tr>
                        <td>ResNet</td>
                        <td>0.9934</td>
                        <td>0.7599</td>
                        <td><b>0.4163</b></td>
                        <td><b>0.4163</b></td>
                    </tr>
                    </tbody>
                </table>

                <table class="tableizer-table">
                    <caption>All hardened ResNet models vs each type of adversarial attack</caption>
                    <thead>
                    <tr class="tableizer-firstrow">
                        <th></th>
                        <th><b>ResNet</b></th>
                        <th colspan="4" style="text-align: center;">Test Data</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td rowspan="5" class="rotate" style="text-align: center; padding-top: 1em;">Trained Model</td>
                        <td></td>
                        <td>Original</td>
                        <td>FGSM</td>
                        <td>BIM</td>
                        <td>PGD</td>
                    </tr>
                    <tr>
                        <td>Original</td>
                        <td>0.9934</td>
                        <td>0.7559</td>
                        <td>0.4163</td>
                        <td>0.4163</td>
                    </tr>
                    <tr>
                        <td>FGSM</td>
                        <td>0.993</td>
                        <td>0.9771</td>
                        <td>0.9675</td>
                        <td>0.9675</td>
                    </tr>
                    <tr>
                        <td>BIM</td>
                        <td><b>0.9944</b></td>
                        <td>0.9732</td>
                        <td>0.9662</td>
                        <td>0.9662</td>
                    </tr>
                    <tr>
                        <td>PGD</td>
                        <td>0.9933</td>
                        <td><b>0.9751</b></td>
                        <td><b>0.9712</b></td>
                        <td><b>0.9712</b></td>
                    </tr>
                    </tbody>
                </table>

            </div>
            <p>
            </p>

            <table class="tableizer-table" style="width: 90%">
                <caption>Top half: Accuracy of each model on the original testing data and adversarial data</caption>
                <caption>Bottom half: Accuracy difference between original model and hardened model</caption>
                <thead>
                <tr>
                    <th></th>
                    <th colspan="2" style="text-align: center;">FGSM Hardened</th>
                    <th colspan="2" style="text-align: center;">BIM Hardened</th>
                    <th colspan="2" style="text-align: center;">PGD Hardened</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>Model/Accuracy</td>
                    <td>Original Data</td>
                    <td>FGSM Adversarial</td>
                    <td>Original Data</td>
                    <td>BIM Adversarial</td>
                    <td>Original Data</td>
                    <td>PGD Adversarial</td>
                </tr>
                <tr>
                    <td>2-Layer Net</td>
                    <td>0.9737</td>
                    <td>0.8335</td>
                    <td>0.9778</td>
                    <td>0.8168</td>
                    <td>0.978</td>
                    <td>0.815</td>
                </tr>
                <tr>
                    <td>VGG</td>
                    <td>0.9898</td>
                    <td>0.9634</td>
                    <td>0.9925</td>
                    <td><b>0.9703</b></td>
                    <td>0.9906</td>
                    <td>0.9555</td>
                </tr>
                <tr>
                    <td>LeNet</td>
                    <td>0.9903</td>
                    <td>0.94</td>
                    <td>0.9898</td>
                    <td>0.9282</td>
                    <td>0.9895</td>
                    <td>0.927</td>
                </tr>
                <tr>
                    <td>GoogLeNet</td>
                    <td><b>0.9936</b></td>
                    <td>0.9629</td>
                    <td>0.9918</td>
                    <td>0.9531</td>
                    <td>0.9905</td>
                    <td>0.9493</td>
                </tr>
                <tr>
                    <td>ResNet</td>
                    <td>0.993</td>
                    <td><b>0.9713</b></td>
                    <td><b>0.9944</b></td>
                    <td>0.9618</td>
                    <td><b>0.9933</b></td>
                    <td><b>0.9663</b></td>
                </tr>
                <tr>
                    <th colspan="7" style="text-align: center;">Accuracy Difference</th>
                </tr>
                <tr>
                    <td>2-Layer Net</td>
                    <td>-0.0147</td>
                    <td>0.3084</td>
                    <td>-0.0106</td>
                    <td>0.6271</td>
                    <td>-0.0104</td>
                    <td>0.6253</td>
                </tr>
                <tr>
                    <td>VGG</td>
                    <td>-0.0053</td>
                    <td>0.1769</td>
                    <td>-0.0026</td>
                    <td>0.7451</td>
                    <td>-0.0045</td>
                    <td>0.7303</td>
                </tr>
                <tr>
                    <td>LeNet</td>
                    <td>-0.0024</td>
                    <td>0.1216</td>
                    <td>-0.0029</td>
                    <td>0.6209</td>
                    <td>-0.0032</td>
                    <td>0.6197</td>
                </tr>
                <tr>
                    <td>GoogLeNet</td>
                    <td><b>-0.0004</b></td>
                    <td>0.1806</td>
                    <td>-0.0022</td>
                    <td>0.5872</td>
                    <td>-0.0035</td>
                    <td>0.8149</td>
                </tr>
                <tr>
                    <td>ResNet</td>
                    <td><b>-0.0004</b></td>
                    <td>0.2114</td>
                    <td><b>0.001</b></td>
                    <td>0.5455</td>
                    <td>-0.0001</td>
                    <td><b>0.2114</b></td>
                </tr>
                </tbody>
            </table>

            <p>

            </p>

            <p>Based on the above tables, here are some key insights:</p>
            <ol>
                <li>
                    The first table shows that models without adversarial training (Original Data) achieve high accuracy
                    on clean examples but perform poorly on adversarial examples, highlighting the vulnerability of
                    standard
                    models to adversarial attacks
                </li>
                <li>ResNet appears to be the most robust model against adversarial attacks across all
                    hardening methods (FGSM, BIM, PGD). It has the smallest accuracy drop on original
                    data compared to the hardened models, and the highest accuracy on adversarial examples.
                </li>
                <li>GoogLeNet also performs relatively well, especially on the FGSM adversarial examples where
                    it has the second highest accuracy of 0.9629. However, ResNet outperforms it on the BIM and
                    PGD adversarial sets.
                </li>
                <li>The 2-Layer Net model seems to be the least robust, having the
                    largest accuracy drops when evaluated on adversarial examples compared to original data.
                </li>
                <li>
                    Adversarial training with PGD hardening appears to provide the best robustness across models,
                    resulting in higher accuracies on PGD adversarial sets compared to FGSM and BIM for each model.
                </li>
                <li>VGG and LeNet have moderate robustness, performing better than the 2-Layer Net but worse than
                    GoogLeNet and ResNet on adversarial examples.
                </li>
                <li>For the more complex GoogLeNet and ResNet models, the accuracy drop on original data after
                    hardening is very small (0.0004), indicating that adversarial training did not significantly
                    impact their performance on clean data.
                </li>
            </ol>
            <p>In summary, the table suggests that more complex models like ResNet and GoogLeNet, coupled with strong
                adversarial training methods like PGD hardening, can provide significant robustness against adversarial
                attacks while maintaining high accuracy on original data.</p>

            <h2>Conclusions</h2>
            <p>
                The network architecture plays a crucial role in robustness to adversarial attacks. Deeper and more
                complex
                models like ResNet and GoogLeNet exhibit higher robustness compared to simpler architectures like the
                2-Layer
                Net and LeNet, likely due to their increased capacity and flexibility to learn robust feature
                representations
                during adversarial training. Architectural elements such as residual connections in ResNet and skip
                connections
                in GoogLeNet's Inception modules contribute to maintaining performance on clean examples while learning
                robustness against adversarial perturbations. In contrast, simple feed-forward architectures struggle to
                learn
                robustness without compromising accuracy on clean data. While depth helps, architectural inductive
                biases
                beyond just stacking convolutions, as seen in the VGG results, may be necessary for robustness. The
                architectural design choices provide effective inductive biases that facilitate learning robust
                representations
                against adversarial attacks during training.
            </p>
            <h4>Error Analysis</h4>
            <p>
                The error analysis of the models revealed that the most common misclassifications occurred between
                visually similar digits, such as 4 and 9, 3 and 8, 7 and 1, and 5 and 6. These pairs of digits have similar
                structures and can be easily confused by the model, especially when the input is perturbed by an
                adversarial attack. We examined two types of errors: those that were misclassified by all four models, and
                those that were correctly classified by the original model but misclassified by the hardened models.
                For both of these error types, the misclassified samples were often visually ambiguous and difficult to
                classify even for humans. This suggests some type of irreducible error that is inherent to the dataset itself.
            </p>
            <figure>
                <img src="figures/testing/misclassified_by_hardened.png" alt="misclassified by hardened" style="max-width: 100%;"/>
                <figcaption style="text-align: center;">Examples of digits misclassified by the hardened models but correctly classified by the original model</figcaption>
            </figure>
            <figure style="max-width: 100%;">
                <img src="figures/testing/misclassified_by_all.png" alt="misclassified by all" style="max-width: 100%;"/>
                <figcaption style="text-align: center;">Examples of digits misclassified by all models</figcaption>
            </figure>
            <p>
            </p>

            <h4>Future Work</h4>
            <p>Adversarial training techniques could be extended to broader applications and larger, more diverse
                datasets
                beyond image classification tasks. As neural network architectures become more advanced, with
                innovations
                like transformers and diffusion models, it will be crucial to investigate their robustness against
                sophisticated
                adversarial attacks tailored to these new architectures. A particularly concerning area is the potential
                to
                apply adversarial attacks to large language models (LLMs) with the goal of tricking them into generating
                harmful or restricted content that they have been specifically trained not to produce. Developing robust
                LLMs
                resilient to such adversarial prompts will be critical for ensuring the safe and responsible deployment
                of
                these powerful AI systems across various domains.
            </p>

            <h3 class="references">References</h3>

            <p><a name="Adversarial Attack Resistance">[1]</a> <a href="https://arxiv.org/pdf/1706.06083.pdf"
            >L&eacute; Madry et al.
                <em>Towards Deep Learning Models Resistant to Adversarial Attacks</em></a>
                arXiv:1706.06083v4 [stat.ML] 4 Sep 2019
            </p>

            <p><a name="DeepFool">[2]</a> <a href="https://arxiv.org/pdf/1511.04599.pdf"
            >L&eacute; Moosavi-Dezfooli et al.
                <em>DeepFool: a simple and accurate method to fool deep neural networks</em></a>
                arXiv:1511.04599 [cs.LG] 4 Jul 2016
            </p>

            <h2>Team Members</h2>

            <p>
            <ul>
                <li>
                    David Pogrebitskiy <a
                        href="mailto:pogrebitskiy.d@northeastern.edu">pogrebitskiy.d@northeastern.edu</a>
                </li>
                <li>
                    Benjamin Wyant <a href="mailto:wyant.b@northeastern.edu">wyant.b@northeastern.edu</a>
                </li>
            </ul>
            </p>


        </div><!--col-->
    </div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
    <div class="row">
        <div class="col-6 col-md text-center">
            <a href="https://cs7150.baulab.info/">About CS 4440</a>
        </div>
    </div>
</footer>

</body>
<script>
    $(document).on('click', '.clickselect', function (ev) {
        var range = document.createRange();
        range.selectNodeContents(this);
        var sel = window.getSelection();
        sel.removeAllRanges();
        sel.addRange(range);
    });
    // Google analytics below.
    window.dataLayer = window.dataLayer || [];
</script>
</html>
